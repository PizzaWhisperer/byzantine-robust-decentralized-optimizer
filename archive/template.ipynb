{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd9a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from codes.worker import SGDMWorker\n",
    "from codes.aggregator import get_aggregator\n",
    "from codes.graph_utils import get_graph\n",
    "from codes.sampler import get_sampler_callback\n",
    "from codes.simulator import DecentralizedTrainer, AverageEvaluator, Evaluator\n",
    "from codes.utils import initialize_logger\n",
    "from codes.tasks.async_loader import AsyncDataLoaderCoordinator\n",
    "from codes.utils import top1_accuracy\n",
    "\n",
    "from codes.tasks.cifar10 import cifar10\n",
    "from codes.tasks.vgg import vgg11\n",
    "from codes.tasks.mnist import Net, mnist\n",
    "from codes.tasks.quadratics import LinearModel, get_distributed_quadratics_tasks\n",
    "from codes.sampler import DistributedSampler\n",
    "\n",
    "def check_noniid_hooks(trainer, E, B):\n",
    "    if E == 1 and B == 0:\n",
    "        lg = trainer.debug_logger\n",
    "        lg.info(f\"\\n=== Peeking data label distribution E{E}B{B} ===\")\n",
    "        for w in trainer.workers:\n",
    "            lg.info(f\"Worker {w.index} has targets: {w.running['target'][:5]}\")\n",
    "        lg.info(\"\\n\")\n",
    "\n",
    "\n",
    "class TaskDef(object):\n",
    "    @staticmethod\n",
    "    def model(device):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def metrics(device):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_func(device):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def train_loader(args, data_dir, sampler, loader_kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def test_loader(args, data_dir, loader_kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class CIFAR10Task(TaskDef):\n",
    "    @staticmethod\n",
    "    def model(device):\n",
    "        return vgg11().to(device)\n",
    "\n",
    "    @staticmethod\n",
    "    def metrics():\n",
    "        return {\"top1\": top1_accuracy}\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_func(device):\n",
    "        return CrossEntropyLoss().to(device)\n",
    "\n",
    "    @staticmethod\n",
    "    def train_loader(args, data_dir, sampler, loader_kwargs):\n",
    "        return cifar10(\n",
    "            data_dir=data_dir,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            batch_size=args.batch_size,\n",
    "            sampler_callback=sampler,\n",
    "            dataset_cls=datasets.CIFAR10,\n",
    "            drop_last=True,  # Exclude the influence of non-full batch.\n",
    "            **loader_kwargs,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def test_loader(args, data_dir, loader_kwargs):\n",
    "        return cifar10(\n",
    "            data_dir=data_dir,\n",
    "            train=False,\n",
    "            download=True,\n",
    "            batch_size=args.test_batch_size,\n",
    "            dataset_cls=datasets.CIFAR10,\n",
    "            shuffle=False,\n",
    "            **loader_kwargs,\n",
    "        )\n",
    "\n",
    "class RunnerTemplate(object):\n",
    "    \n",
    "    use_cuda = False\n",
    "    debug = False\n",
    "    seed = 0\n",
    "    log_interval = 10\n",
    "    identifier = \"debug\"\n",
    "    analyze = False\n",
    "\n",
    "    #default variables\n",
    "    lr = 0.01\n",
    "    momentum = 0.9\n",
    "    noniid = 0 #0 = iid, 1 = non iid\n",
    "    longtail = 0 #0 for not-longtail and 1 for longtail\n",
    "    agg = \"avg\"\n",
    "\n",
    "    n = 6 #Number of workers\n",
    "    f = 2 #Number of Byzantine workers\n",
    "\n",
    "    epochs = 100\n",
    "    graph = \"complete\"\n",
    "\n",
    "    attack = RandomAttack\n",
    "\n",
    "    batch_size = 32\n",
    "    test_batch_size = 128\n",
    "    max_batch_size_per_epoch = 999999999\n",
    "\n",
    "    ROOT_DIR = os.path.dirname(os.path.abspath(__file__)) + \"/\"\n",
    "    DATA_DIR = ROOT_DIR + \"datasets/\"\n",
    "\n",
    "    # Pattern of the experiment output director\n",
    "    EXP_PATTERN = \"f{f}m{momentum}n{n}_noniid{noniid}_graph{graph}_agg{agg}\"\n",
    "    LOG_DIR_PATTERN = ROOT_DIR + \\\n",
    "        \"outputs/{script}/{exp_id}/\" + EXP_PATTERN + \"/\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 parser_func,\n",
    "                 trainer_fn,\n",
    "                 sampler_fn,\n",
    "                 lr_scheduler_fn,\n",
    "                 task,\n",
    "                 worker_fn,\n",
    "                 evaluators_fn,\n",
    "                 get_graph=get_graph,\n",
    "                 get_aggregator=get_aggregator):\n",
    "        parser = parser_func()\n",
    "        self.args = self.parse_arguments(parser)\n",
    "        self.check_arguments(self.args)\n",
    "        self.setup(self.args)\n",
    "\n",
    "        self.task = task\n",
    "        self.sampler_fn = sampler_fn\n",
    "        self.trainer_fn = trainer_fn\n",
    "        self.lr_scheduler_fn = lr_scheduler_fn\n",
    "        self.worker_fn = worker_fn\n",
    "        self.evaluators_fn = evaluators_fn\n",
    "        self.get_graph = codes.graph_utils.get_graph\n",
    "        self.get_aggregator = get_aggregator\n",
    "\n",
    "    def run(self):\n",
    "        if self.args.analyze:\n",
    "            self.generate_analysis()\n",
    "        else:\n",
    "            self.train()\n",
    "\n",
    "    def generate_analysis(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self):\n",
    "        args = self.args\n",
    "        device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
    "        kwargs = {\"pin_memory\": True}\n",
    "        graph = self.get_graph(args)\n",
    "        trainer = self.trainer_fn(args, self.task.metrics())\n",
    "        model = self.task.model(device)\n",
    "        loader_coordinator = AsyncDataLoaderCoordinator(device=device)\n",
    "\n",
    "        trainer.debug_logger.info(\"\\n=== Start adding workers ===\")\n",
    "        lr_schedulers = []\n",
    "        for rank in range(args.n):\n",
    "            sampler = self.sampler_fn(args, rank)\n",
    "            train_loader = self.task.train_loader(\n",
    "                args, data_dir=self.DATA_DIR, sampler=sampler, loader_kwargs=kwargs\n",
    "            )\n",
    "            m = copy.deepcopy(model).to(device)\n",
    "\n",
    "            # NOTE: for the moment, we fix this to be SGD\n",
    "            optimizer = torch.optim.SGD(m.parameters(), lr=args.lr)\n",
    "            lr_scheduler = self.lr_scheduler_fn(optimizer)\n",
    "            lr_schedulers.append(lr_scheduler)\n",
    "            train_loader = loader_coordinator.add(train_loader)\n",
    "            loss_func = self.task.loss_func(device)\n",
    "\n",
    "            worker = self.worker_fn(args=args, trainer=trainer, rank=rank, model=m,\n",
    "                                    opt=optimizer, loss_func=loss_func, m=args.momentum,\n",
    "                                    loader=train_loader, device=device, lr_scheduler=lr_scheduler)\n",
    "\n",
    "            trainer.add_worker(worker, self.get_aggregator(\n",
    "                args, graph, rank, worker))\n",
    "\n",
    "        trainer.add_graph(graph)\n",
    "        test_loader = self.task.test_loader(args, self.DATA_DIR, kwargs)\n",
    "        evaluators = self.evaluators_fn(\n",
    "            args, self.task, trainer, test_loader, device)\n",
    "\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            trainer.train(epoch)\n",
    "\n",
    "            # Evaluation\n",
    "            for evaluator in evaluators:\n",
    "                evaluator.evaluate(epoch)\n",
    "\n",
    "            # Update resampler and lr_schedulers\n",
    "            if hasattr(trainer.workers[0], \"sampler\") and isinstance(\n",
    "                trainer.workers[0].sampler, DistributedSampler\n",
    "            ):\n",
    "                trainer.decall(\n",
    "                    lambda w: w.data_loader.sampler.set_epoch(epoch))\n",
    "            for scheduler in lr_schedulers:\n",
    "                scheduler.step()\n",
    "\n",
    "    # ---------------------------------------------------------------------------- #\n",
    "    #                                Parse arguments                               #\n",
    "    # ---------------------------------------------------------------------------- #\n",
    "\n",
    "    def parse_arguments(self, parser):\n",
    "        if len(sys.argv) > 1:\n",
    "            return parser.parse_args()\n",
    "        return parser.parse_args(self.DEFAULT_LINE_ARG.split())\n",
    "\n",
    "    def check_arguments(self, args):\n",
    "        assert args.n > 0\n",
    "        assert args.epochs >= 1\n",
    "\n",
    "    # ---------------------------------------------------------------------------- #\n",
    "    #                               Setup experiments                              #\n",
    "    # ---------------------------------------------------------------------------- #\n",
    "    def setup(self, args):\n",
    "        self._setup_logs(args)\n",
    "\n",
    "        torch.manual_seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "\n",
    "    def _setup_logs(self, args):\n",
    "        assert \"script\" not in args.__dict__\n",
    "        assert \"exp_id\" not in args.__dict__\n",
    "        log_dir = self.LOG_DIR_PATTERN.format(\n",
    "            script=sys.argv[0][:-3],\n",
    "            exp_id=args.identifier,\n",
    "            # NOTE: Customize the hp\n",
    "            **args.__dict__\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        if not args.analyze:\n",
    "            initialize_logger(log_dir)\n",
    "            with open(os.path.join(log_dir, \"args.json\"), \"w\") as f:\n",
    "                json.dump(args.__dict__, f)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "#                            CIFAR10 Runner Example                            #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "class CIFAR10_Template(RunnerTemplate):\n",
    "    \"\"\"\n",
    "    The default setup with VGG 11 yields (88.38)% accuracy after 150 epochs.\n",
    "\n",
    "    Setups:\n",
    "    - n=4 + fully connected + iid + gossip avg\n",
    "    - momentum=0.9 batchsize=128\n",
    "    - epochs=150\n",
    "    - no weight decay (maybe add one)\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_LINE_ARG = \"\"\"--lr 0.05 --use-cuda --debug -n 4 -f 0 --epochs 150 --momentum 0.9 \\\n",
    "--batch-size 128 --max-batch-size-per-epoch 9999 --graph complete --noniid 0 --agg gossip_avg \\\n",
    "--identifier vgg11\n",
    "\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 parser_func=define_parser,\n",
    "                 trainer_fn=lambda args, metrics: DecentralizedTrainer(\n",
    "                     pre_batch_hooks=[],\n",
    "                     post_batch_hooks=[check_noniid_hooks],\n",
    "                     max_batches_per_epoch=args.max_batch_size_per_epoch,\n",
    "                     log_interval=args.log_interval,\n",
    "                     metrics=metrics,\n",
    "                     use_cuda=args.use_cuda,\n",
    "                     debug=args.debug,\n",
    "                 ),\n",
    "                 sampler_fn=lambda args, rank: get_sampler_callback(\n",
    "                     rank, args.n, noniid=args.noniid, longtail=args.longtail),\n",
    "                 lr_scheduler_fn=lambda opt: torch.optim.lr_scheduler.MultiStepLR(\n",
    "                     opt, milestones=list(range(30, 300, 30)), gamma=0.5),\n",
    "                 task=CIFAR10Task,\n",
    "                 worker_fn=lambda args, trainer, rank, model, opt, loss_func, m, loader, device, lr_scheduler: SGDMWorker(\n",
    "                     momentum=m,\n",
    "                     index=rank,\n",
    "                     data_loader=loader,\n",
    "                     model=model,\n",
    "                     optimizer=opt,\n",
    "                     loss_func=loss_func,\n",
    "                     device=device,\n",
    "                     lr_scheduler=lr_scheduler),\n",
    "                 evaluators_fn=lambda args, task, trainer, test_loader, device: [\n",
    "                     AverageEvaluator(\n",
    "                         models=[w.model for w in trainer.workers],\n",
    "                         data_loader=test_loader,\n",
    "                         loss_func=task.loss_func(device),\n",
    "                         device=device,\n",
    "                         metrics=task.metrics(),\n",
    "                         use_cuda=args.use_cuda,\n",
    "                         debug=args.debug,\n",
    "                     )\n",
    "                 ],\n",
    "                 get_graph=get_graph,\n",
    "                 get_aggregator=get_aggregator):\n",
    "        super().__init__(\n",
    "            parser_func=parser_func,\n",
    "            trainer_fn=trainer_fn,\n",
    "            sampler_fn=sampler_fn,\n",
    "            lr_scheduler_fn=lr_scheduler_fn,\n",
    "            task=task,\n",
    "            worker_fn=worker_fn,\n",
    "            evaluators_fn=evaluators_fn,\n",
    "            get_graph=get_graph,\n",
    "            get_aggregator=get_aggregator\n",
    "        )\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "#                             MNIST Runner example                             #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "class MNISTTask(TaskDef):\n",
    "    @staticmethod\n",
    "    def model(device):\n",
    "        return Net().to(device)\n",
    "\n",
    "    @staticmethod\n",
    "    def metrics():\n",
    "        return {\"top1\": top1_accuracy}\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_func(device):\n",
    "        return F.nll_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def train_loader(args, data_dir, sampler, loader_kwargs):\n",
    "        return mnist(\n",
    "            data_dir=data_dir,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            batch_size=args.batch_size,\n",
    "            sampler_callback=sampler,\n",
    "            dataset_cls=datasets.MNIST,\n",
    "            drop_last=True,  # Exclude the influence of non-full batch.\n",
    "            **loader_kwargs,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def test_loader(args, data_dir, loader_kwargs):\n",
    "        return mnist(\n",
    "            data_dir=data_dir,\n",
    "            train=False,\n",
    "            download=True,\n",
    "            batch_size=args.test_batch_size,\n",
    "            dataset_cls=datasets.MNIST,\n",
    "            shuffle=False,\n",
    "            **loader_kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "class MNISTTemplate(RunnerTemplate):\n",
    "    \"\"\"\n",
    "    Accuracy 98.48%\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_LINE_ARG = \"\"\"--lr 0.01 --use-cuda --debug -n 8 -f 0 --epochs 30 --momentum 0.0 \\\n",
    "--batch-size 32 --max-batch-size-per-epoch 9999 --graph complete --noniid 0 --agg gossip_avg \\\n",
    "--identifier mnist\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 trainer_fn=lambda args, metrics: DecentralizedTrainer(\n",
    "                     pre_batch_hooks=[],\n",
    "                     post_batch_hooks=[check_noniid_hooks],\n",
    "                     max_batches_per_epoch=args.max_batch_size_per_epoch,\n",
    "                     log_interval=args.log_interval,\n",
    "                     metrics=metrics,\n",
    "                     use_cuda=args.use_cuda,\n",
    "                     debug=args.debug,\n",
    "                 ),\n",
    "                 sampler_fn=lambda args, rank: get_sampler_callback(\n",
    "                     rank, args.n, noniid=args.noniid, longtail=args.longtail),\n",
    "                 lr_scheduler_fn=lambda opt: torch.optim.lr_scheduler.MultiStepLR(\n",
    "                     opt, milestones=[], gamma=1.0),\n",
    "                 task=MNISTTask,\n",
    "                 worker_fn=lambda args, trainer, rank, model, opt, loss_func, m, loader, device, lr_scheduler: SGDMWorker(\n",
    "                     momentum=m,\n",
    "                     index=rank,\n",
    "                     data_loader=loader,\n",
    "                     model=model,\n",
    "                     optimizer=opt,\n",
    "                     loss_func=loss_func,\n",
    "                     device=device,\n",
    "                     lr_scheduler=lr_scheduler),\n",
    "                 evaluators_fn=lambda args, task, trainer, test_loader, device: [\n",
    "                     AverageEvaluator(\n",
    "                         models=[w.model for w in trainer.workers],\n",
    "                         data_loader=test_loader,\n",
    "                         loss_func=task.loss_func(device),\n",
    "                         device=device,\n",
    "                         metrics=task.metrics(),\n",
    "                         use_cuda=args.use_cuda,\n",
    "                         debug=args.debug,\n",
    "                     )\n",
    "                 ],\n",
    "                 get_graph=get_graph,\n",
    "                 get_aggregator=get_aggregator):\n",
    "        super().__init__(\n",
    "            parser_func=parser_func,\n",
    "            trainer_fn=trainer_fn,\n",
    "            sampler_fn=sampler_fn,\n",
    "            lr_scheduler_fn=lr_scheduler_fn,\n",
    "            task=task,\n",
    "            worker_fn=worker_fn,\n",
    "            evaluators_fn=evaluators_fn,\n",
    "            get_graph=get_graph,\n",
    "            get_aggregator=get_aggregator\n",
    "        )\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "#                              Quadratics Problem                              #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "def define_parser_quadratics():\n",
    "    parser = define_parser()\n",
    "\n",
    "    # NOTE: customize per script\n",
    "    parser.add_argument(\"--n-samples-per_worker\", type=int, default=100)\n",
    "    parser.add_argument(\"-d\", type=int, default=10)\n",
    "    parser.add_argument(\"-L\", type=float, default=30.0)\n",
    "    parser.add_argument(\"--mu\", type=float, default=-1.0)\n",
    "    parser.add_argument(\"--r0\", type=float, default=10.0)\n",
    "    parser.add_argument(\"--sigma\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--zeta\", type=float, default=1.0)\n",
    "\n",
    "    parser.add_argument(\"--comm-rounds\", type=int, default=1)\n",
    "    return parser\n",
    "\n",
    "\n",
    "class QuadraticsTask(TaskDef):\n",
    "    def __init__(self, args, tasks, main_task):\n",
    "        self.args = args\n",
    "        self.tasks = tasks\n",
    "        self.main_task = main_task\n",
    "\n",
    "    def model(self, device):\n",
    "        model = LinearModel(self.args.d)\n",
    "        model.layer.weight.data /= model.layer.weight.data.norm() / self.args.r0\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def metrics():\n",
    "        return {}\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_func(device):\n",
    "        return torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "    def train_loader(self, args, data_dir, sampler, loader_kwargs):\n",
    "        rank = sampler\n",
    "        return self.tasks[rank].train_loader()\n",
    "\n",
    "    def test_loader(self, args, data_dir, loader_kwargs):\n",
    "        return self.main_task.test_loader()\n",
    "\n",
    "\n",
    "class QuadraticsTemplate(RunnerTemplate):\n",
    "    \"\"\"\n",
    "    Setups:\n",
    "    \"\"\"\n",
    "\n",
    "    #DEFAULT_LINE_ARG = \"\"\"--debug -n 16 -f 0 --epochs 100 --momentum 0 --batch-size 100 -d 10 --n-samples-per_worker 200 \\\n",
    "#-L 30.0 --mu -1.0 --r0 10.0 --sigma 0.0 --zeta 0.0 \\\n",
    "#\n",
    "--graph torusC4C4 --agg gossip_avg --identifier quadratics\"\"\"\n",
    "\n",
    "    EXP_PATTERN = \"f{f}m{momentum}n{n}graph{graph}_agg{agg}_d{d}_L{L}_mu{mu}_r0{r0}_sigma{sigma}_zeta{zeta}\"\n",
    "    LOG_DIR_PATTERN = RunnerTemplate.ROOT_DIR + \\\n",
    "        \"outputs/{script}/{exp_id}/\" + EXP_PATTERN + \"/\"\n",
    "\n",
    "    def __init__(self):\n",
    "        def trainer_fn(args, metrics): return DecentralizedTrainer(\n",
    "            pre_batch_hooks=[],\n",
    "            post_batch_hooks=[check_noniid_hooks],\n",
    "            max_batches_per_epoch=args.max_batch_size_per_epoch,\n",
    "            log_interval=args.log_interval,\n",
    "            metrics=metrics,\n",
    "            use_cuda=args.use_cuda,\n",
    "            debug=args.debug,\n",
    "        )\n",
    "\n",
    "        def sampler_fn(args, rank):\n",
    "            return rank\n",
    "\n",
    "        def lr_scheduler_fn(opt): return torch.optim.lr_scheduler.MultiStepLR(\n",
    "            opt, milestones=[], gamma=1.0)\n",
    "\n",
    "        def worker_fn(args, trainer, rank, model, opt, loss_func, m, loader, device): return SGDMWorker(\n",
    "            momentum=m,\n",
    "            index=rank,\n",
    "            data_loader=loader,\n",
    "            model=model,\n",
    "            optimizer=opt,\n",
    "            loss_func=loss_func,\n",
    "            device=device)\n",
    "\n",
    "        def evaluators_fn(args, task, trainer, test_loader, device): return [\n",
    "            AverageEvaluator(\n",
    "                models=[w.model for w in trainer.workers],\n",
    "                data_loader=test_loader,\n",
    "                loss_func=task.loss_func(device),\n",
    "                device=device,\n",
    "                metrics=task.metrics(),\n",
    "                use_cuda=args.use_cuda,\n",
    "                debug=args.debug,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        #####################################################\n",
    "        parser = define_parser_quadratics()\n",
    "        args = self.parse_arguments(parser)\n",
    "        self.args = args\n",
    "        self.check_arguments(self.args)\n",
    "        self.setup(self.args)\n",
    "        args.lr = 1 / args.L * 0.5\n",
    "\n",
    "        tasks, main_task = get_distributed_quadratics_tasks(\n",
    "            m=args.n,\n",
    "            n=args.n_samples_per_worker,\n",
    "            d=args.d,\n",
    "            b=args.batch_size,\n",
    "            L=args.L,\n",
    "            mu=args.mu,\n",
    "            r0=args.r0,\n",
    "            sigma=args.sigma,\n",
    "            zeta=args.zeta,\n",
    "            seed=args.seed,\n",
    "        )\n",
    "\n",
    "        self.task = QuadraticsTask(args, tasks, main_task)\n",
    "\n",
    "        self.sampler_fn = sampler_fn\n",
    "        self.trainer_fn = trainer_fn\n",
    "        self.lr_scheduler_fn = lr_scheduler_fn\n",
    "        self.worker_fn = worker_fn\n",
    "        self.evaluators_fn = evaluators_fn\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    runner = QuadraticsTemplate()\n",
    "    runner.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
